{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MivJxcN7Ezh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import MultivariateNormal\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "    \n",
        "    def clear_memory(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.is_terminals[:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1S7OXCOE4IM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, action_std):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        # action mean range -1 to 1\n",
        "        self.actor =  nn.Sequential(\n",
        "                nn.Linear(state_dim, 64),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(64, 32),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(32, action_dim),\n",
        "                nn.Tanh()\n",
        "                )\n",
        "        # critic\n",
        "        self.critic = nn.Sequential(\n",
        "                nn.Linear(state_dim, 64),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(64, 32),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(32, 1)\n",
        "                )\n",
        "        self.action_var = torch.full((action_dim,), action_std*action_std).to(device)\n",
        "        \n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def act(self, state, memory):\n",
        "        action_mean = self.actor(state)\n",
        "        cov_mat = torch.diag(self.action_var).to(device)\n",
        "        \n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        \n",
        "        memory.states.append(state)\n",
        "        memory.actions.append(action)\n",
        "        memory.logprobs.append(action_logprob)\n",
        "        \n",
        "        return action.detach()\n",
        "    \n",
        "    def evaluate(self, state, action):   \n",
        "        action_mean = self.actor(state)\n",
        "        \n",
        "        action_var = self.action_var.expand_as(action_mean)\n",
        "        cov_mat = torch.diag_embed(action_var).to(device)\n",
        "        \n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        \n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_value = self.critic(state)\n",
        "        \n",
        "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n",
        "\n",
        "\n",
        "class ActorCriticLayerNorm(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, action_std):\n",
        "        super(ActorCriticLayerNorm, self).__init__()\n",
        "        # action mean range -1 to 1\n",
        "        self.actor =  nn.Sequential(\n",
        "                nn.Linear(state_dim, 64),\n",
        "                nn.LayerNorm(64),\n",
        "                nn.Tanh(),\n",
        "\n",
        "                nn.Linear(64, 32),\n",
        "                nn.LayerNorm(32),\n",
        "                nn.Tanh(),\n",
        "\n",
        "                nn.Linear(32, action_dim),\n",
        "                nn.Tanh()\n",
        "                )\n",
        "        # critic\n",
        "        self.critic = nn.Sequential(\n",
        "                nn.Linear(state_dim, 64),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(64, 32),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(32, 1)\n",
        "                )\n",
        "        self.action_var = torch.full((action_dim,), action_std*action_std).to(device)\n",
        "        \n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def act(self, state, memory):\n",
        "        action_mean = self.actor(state)\n",
        "        cov_mat = torch.diag(self.action_var).to(device)\n",
        "        \n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        \n",
        "        memory.states.append(state)\n",
        "        memory.actions.append(action)\n",
        "        memory.logprobs.append(action_logprob)\n",
        "        \n",
        "        return action.detach()\n",
        "    \n",
        "    def evaluate(self, state, action):   \n",
        "        action_mean = self.actor(state)\n",
        "        \n",
        "        action_var = self.action_var.expand_as(action_mean)\n",
        "        cov_mat = torch.diag_embed(action_var).to(device)\n",
        "        \n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        \n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_value = self.critic(state)\n",
        "        \n",
        "        return action_logprobs, torch.squeeze(state_value), dist_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeEG5yT0FocB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PPO:\n",
        "    def __init__(self,state_dim,action_dim,n_latent_var,K_epochs,parameter_noise):\n",
        "        self.lr = 0.002\n",
        "        self.betas = (0.9,0.999)\n",
        "        self.gamma =0.99\n",
        "        self.eps_clip = 0.2\n",
        "\n",
        "        self.K_epochs = K_epochs\n",
        "\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "        self.action_std = 0.5\n",
        "\n",
        "        if parameter_noise:\n",
        "            self.policy_new = ActorCriticLayerNorm(state_dim,action_dim,self.action_std).to(device)\n",
        "            self.policy_old = ActorCriticLayerNorm(state_dim,action_dim,self.action_std).to(device)\n",
        "\n",
        "        else:\n",
        "\n",
        "            #we have 2 policies old and new\n",
        "            self.policy_new = ActorCritic(state_dim,action_dim,self.action_std).to(device)\n",
        "            self.policy_old = ActorCritic(state_dim,action_dim,self.action_std).to(device)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.policy_new.parameters(),lr = self.lr,betas = self.betas)\n",
        "\n",
        "        #synchronizing 2 neural networks in the beginning\n",
        "        self.policy_old.load_state_dict(self.policy_new.state_dict())\n",
        "\n",
        "    def select_action(self, state, memory):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        return self.policy_old.act(state, memory).cpu().data.numpy().flatten()\n",
        "\n",
        "    def update(self,memory):\n",
        "        \n",
        "        gamma = self.gamma\n",
        "        #creating list of discounted rewards\n",
        "        discounted_rewards = []\n",
        "        discounted_reward = 0\n",
        "        \n",
        "        for reward, is_terminal in zip(reversed(memory.rewards),reversed(memory.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + gamma * discounted_reward\n",
        "            discounted_rewards.append(discounted_reward)\n",
        "\n",
        "        discounted_rewards = discounted_rewards[::-1]\n",
        "\n",
        "        discounted_rewards = torch.tensor(discounted_rewards).to(device)\n",
        "        #normalizing: mean = 0, std = 1\n",
        "        discounted_rewards = (discounted_rewards - torch.mean(discounted_rewards))/(torch.std(discounted_rewards) + 1e-5)\n",
        "        \n",
        "\n",
        "        #creating tensor from list\n",
        "        old_states = torch.stack(memory.states).to(device).detach()\n",
        "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
        "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
        "\n",
        "        #gradient ascent for K epochs\n",
        "        for i in range(self.K_epochs):\n",
        "\n",
        "            logprobs, state_values, dist_entropy = self.policy_new.evaluate(old_states,old_actions)\n",
        "\n",
        "            #calculating policy_new/policy_old ratio\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            #surrogate loss\n",
        "            advantages = discounted_rewards - state_values\n",
        "\n",
        "            #\n",
        "            surr1 = advantages * ratios\n",
        "            surr2 = torch.clamp(ratios,min=1-self.eps_clip,max = 1+self.eps_clip)*advantages\n",
        "\n",
        "            loss = -torch.min(surr1,surr2) + 0.5 * self.mse_loss(discounted_rewards,state_values) - 0.01*dist_entropy\n",
        "\n",
        "            #backprop\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "\n",
        "        #new policy becomes old policy\n",
        "        self.policy_old.load_state_dict(self.policy_new.state_dict())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ucWccHcLC_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_noise(policy, std):\n",
        "    n_normalized_layers = 2\n",
        "    noisy_layers=0\n",
        "    for layer in policy.actor:\n",
        "        if noisy_layers == n_normalized_layers:\n",
        "            break\n",
        "        if isinstance(layer,nn.Linear):\n",
        "            #generating noise with given std\n",
        "            noise = torch.normal(mean=0,std=std,size=(layer.weight.shape)).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                layer.weight += noise\n",
        "            noisy_layers += 1\n",
        "    # we don't have to return anything because function modifies parameters in place"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LowNMw2YE8GV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(parameter_noise=False):\n",
        "    ############## Hyperparameters ##############\n",
        "    env_name = \"Pendulum-v0\"\n",
        "    render = False\n",
        "    solved_reward = 300         # stop training if avg_reward > solved_reward\n",
        "    log_interval = 100           # print avg reward in the interval\n",
        "    max_episodes = 5000        # max training episodes\n",
        "    max_timesteps = 200        # max timesteps in one episode\n",
        "    \n",
        "    update_timestep = 2000      # update policy every n timesteps\n",
        "    action_std = 0.5            # constant std for action distribution (Multivariate Normal)\n",
        "    K_epochs = 2               # update policy for K epochs\n",
        "    eps_clip = 0.2              # clip parameter for PPO\n",
        "    gamma = 0.99                # discount factor\n",
        "    n_latent_var = 64\n",
        "    lr = 0.0003                 # parameters for Adam optimizer\n",
        "    betas = (0.9, 0.999)\n",
        "    \n",
        "    random_seed = None\n",
        "    #############################################\n",
        "    \n",
        "    # creating environment\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    \n",
        "    if random_seed:\n",
        "        print(\"Random Seed: {}\".format(random_seed))\n",
        "        torch.manual_seed(random_seed)\n",
        "        env.seed(random_seed)\n",
        "        np.random.seed(random_seed)\n",
        "    \n",
        "    memory = Memory()\n",
        "    ppo = PPO(state_dim, action_dim,n_latent_var, K_epochs,parameter_noise)\n",
        "    print(lr,betas)\n",
        "    \n",
        "    # logging variables\n",
        "    running_reward = 0\n",
        "    avg_length = 0\n",
        "    time_step = 0\n",
        "\n",
        "\n",
        "    #parameter noise hyperparams\n",
        "    sigma = 0.1\n",
        "    distance_threshold = 0.2\n",
        "    noise_scalefactor = 1.01\n",
        "    perturbation_timestep = 4000\n",
        "\n",
        "    rewards_plot = []\n",
        "    \n",
        "\n",
        "    # training loop\n",
        "    for i_episode in range(1, max_episodes+1):\n",
        "        state = env.reset()\n",
        "\n",
        "        add_noise(ppo.policy_old,sigma)\n",
        "\n",
        "\n",
        "\n",
        "        for t in range(max_timesteps):\n",
        "            time_step +=1\n",
        "            # Running policy_old:\n",
        "            action = ppo.select_action(state, memory)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            # Saving reward and is_terminals:\n",
        "            memory.rewards.append(reward)\n",
        "            memory.is_terminals.append(done)\n",
        "            \n",
        "\n",
        "            if time_step % perturbation_timestep ==0 and parameter_noise:\n",
        "                with torch.no_grad():\n",
        "                    # print('parameter noise')\n",
        "                    \n",
        "                    policy_perturbed = ActorCriticLayerNorm(state_dim,action_dim,action_std).to(device)\n",
        "\n",
        "                    policy_perturbed.load_state_dict(ppo.policy_old.state_dict())\n",
        "\n",
        "                    add_noise(policy_perturbed, sigma)\n",
        "\n",
        "                    states = torch.stack(memory.states).to(device).detach()\n",
        "\n",
        "                                        #action means\n",
        "                    out_original = ppo.policy_old.actor(states)\n",
        "                    out_perturbed = policy_perturbed.actor(states)\n",
        "\n",
        "                    distance = torch.mean(torch.sqrt((out_original - out_perturbed)**2))\n",
        "\n",
        "                    if distance < distance_threshold:\n",
        "                            # parameter_noise = [layer_noise * noise_scalefactor for layer_noise in parameter_noise]\n",
        "                        sigma *= noise_scalefactor\n",
        "\n",
        "                    elif distance > distance_threshold:\n",
        "                            # parameter_noise = [layer_noise / noise_scalefactor for layer_noise in parameter_noise]\n",
        "                        sigma /= noise_scalefactor\n",
        "\n",
        "                    # ppo.policy_old.load_state_dict(policy_perturbed.state_dict())\n",
        "\n",
        "\n",
        "\n",
        "            # update if its time\n",
        "            if time_step % update_timestep == 0:\n",
        "                # print('update')\n",
        "                ppo.update(memory)\n",
        "                memory.clear_memory()\n",
        "                # time_step = 0\n",
        "            running_reward += reward\n",
        "            # rewards_plot.append(reward)\n",
        "            if render:\n",
        "                env.render()\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        avg_length += t\n",
        "        \n",
        "        # stop training if avg_reward > solved_reward\n",
        "        if running_reward > (log_interval*solved_reward):\n",
        "            print(\"########## Solved! ##########\")\n",
        "            torch.save(ppo.policy.state_dict(), './PPO_continuous_solved_{}.pth'.format(env_name))\n",
        "            break\n",
        "\n",
        "        \n",
        "            \n",
        "        # logging\n",
        "        if i_episode % log_interval == 0:\n",
        "            avg_length = int(avg_length/log_interval)\n",
        "            running_reward = int((running_reward/log_interval))\n",
        "            rewards_plot.append(running_reward)\n",
        "            \n",
        "            print('Episode {} \\t Avg length: {} \\t Avg reward: {}'.format(i_episode, avg_length, running_reward))\n",
        "            running_reward = 0\n",
        "            avg_length = 0\n",
        "\n",
        "    return rewards_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iNHpMZxE-I9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    rewards_noisy = main(parameter_noise=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjB6W77qBPoy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "3ff8dd3e-8fe1-4e55-b72f-d7dc700cdbbc"
      },
      "source": [
        "rewards_nonoise = main(parameter_noise=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0003 (0.9, 0.999)\n",
            "Episode 100 \t Avg length: 199 \t Avg reward: -1300\n",
            "Episode 200 \t Avg length: 199 \t Avg reward: -1340\n",
            "Episode 300 \t Avg length: 199 \t Avg reward: -1258\n",
            "Episode 400 \t Avg length: 199 \t Avg reward: -1299\n",
            "Episode 500 \t Avg length: 199 \t Avg reward: -1189\n",
            "Episode 600 \t Avg length: 199 \t Avg reward: -1258\n",
            "Episode 700 \t Avg length: 199 \t Avg reward: -1307\n",
            "Episode 800 \t Avg length: 199 \t Avg reward: -1292\n",
            "Episode 900 \t Avg length: 199 \t Avg reward: -1323\n",
            "Episode 1000 \t Avg length: 199 \t Avg reward: -1374\n",
            "Episode 1100 \t Avg length: 199 \t Avg reward: -1340\n",
            "Episode 1200 \t Avg length: 199 \t Avg reward: -1263\n",
            "Episode 1300 \t Avg length: 199 \t Avg reward: -1217\n",
            "Episode 1400 \t Avg length: 199 \t Avg reward: -1254\n",
            "Episode 1500 \t Avg length: 199 \t Avg reward: -1361\n",
            "Episode 1600 \t Avg length: 199 \t Avg reward: -1329\n",
            "Episode 1700 \t Avg length: 199 \t Avg reward: -1265\n",
            "Episode 1800 \t Avg length: 199 \t Avg reward: -1330\n",
            "Episode 1900 \t Avg length: 199 \t Avg reward: -1279\n",
            "Episode 2000 \t Avg length: 199 \t Avg reward: -1230\n",
            "Episode 2100 \t Avg length: 199 \t Avg reward: -1186\n",
            "Episode 2200 \t Avg length: 199 \t Avg reward: -1333\n",
            "Episode 2300 \t Avg length: 199 \t Avg reward: -1218\n",
            "Episode 2400 \t Avg length: 199 \t Avg reward: -1286\n",
            "Episode 2500 \t Avg length: 199 \t Avg reward: -1404\n",
            "Episode 2600 \t Avg length: 199 \t Avg reward: -1295\n",
            "Episode 2700 \t Avg length: 199 \t Avg reward: -1389\n",
            "Episode 2800 \t Avg length: 199 \t Avg reward: -1232\n",
            "Episode 2900 \t Avg length: 199 \t Avg reward: -1272\n",
            "Episode 3000 \t Avg length: 199 \t Avg reward: -1315\n",
            "Episode 3100 \t Avg length: 199 \t Avg reward: -1321\n",
            "Episode 3200 \t Avg length: 199 \t Avg reward: -1334\n",
            "Episode 3300 \t Avg length: 199 \t Avg reward: -1305\n",
            "Episode 3400 \t Avg length: 199 \t Avg reward: -1367\n",
            "Episode 3500 \t Avg length: 199 \t Avg reward: -1277\n",
            "Episode 3600 \t Avg length: 199 \t Avg reward: -1283\n",
            "Episode 3700 \t Avg length: 199 \t Avg reward: -1182\n",
            "Episode 3800 \t Avg length: 199 \t Avg reward: -1180\n",
            "Episode 3900 \t Avg length: 199 \t Avg reward: -1219\n",
            "Episode 4000 \t Avg length: 199 \t Avg reward: -1386\n",
            "Episode 4100 \t Avg length: 199 \t Avg reward: -1352\n",
            "Episode 4200 \t Avg length: 199 \t Avg reward: -1273\n",
            "Episode 4300 \t Avg length: 199 \t Avg reward: -1306\n",
            "Episode 4400 \t Avg length: 199 \t Avg reward: -1309\n",
            "Episode 4500 \t Avg length: 199 \t Avg reward: -1273\n",
            "Episode 4600 \t Avg length: 199 \t Avg reward: -1243\n",
            "Episode 4700 \t Avg length: 199 \t Avg reward: -1334\n",
            "Episode 4800 \t Avg length: 199 \t Avg reward: -1279\n",
            "Episode 4900 \t Avg length: 199 \t Avg reward: -1344\n",
            "Episode 5000 \t Avg length: 199 \t Avg reward: -1294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxwoibBiy654",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "4bda8e2a-bf9e-49ce-cc49-22b91a537cd8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "t = np.arange(len(rewards_noisy))\n",
        "plt.ylabel('Reward')\n",
        "plt.xlabel(\"Episode * 100\")\n",
        "plt.scatter(t,rewards_noisy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f682f09e9b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPdB1XWqA3Cu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "3e3f63d8-223f-48c2-9859-ed8e392a0735"
      },
      "source": [
        "t = np.arange(len(rewards_nonoise))\n",
        "plt.ylabel('Reward')\n",
        "plt.xlabel(\"Episode * 100\")\n",
        "plt.scatter(t,rewards_nonoise)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f682c856cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}